---
path: fr/2.3.-AI-Setup
---

# Configuration de l'IA

Après l'installation de QualCoder, les fonctionnalités améliorées par l'IA nécessitent une configuration supplémentaire. Lors du premier lancement de l'application, un **assistant** apparaît et vous guide tout au long du processus. Si vous choisissez de sauter cette étape, vous pourrez relancer l'assistant plus tard en sélectionnant l'option de menu **« IA > Configuration »**.

Le processus de configuration s'exécute principalement de manière automatique et sans surveillance. Il s'agit d'une opération ponctuelle qui peut prendre un certain temps, soyez patient. Les étapes principales sont les suivantes :

---

### 1. Activer l'IA

* Notez que les fonctionnalités liées à l'IA sont **« opt-in »** et désactivées par défaut.
* Toutes les options liées à l'IA se trouvent en bas de la boîte de dialogue des paramètres. Faites défiler vers le bas si nécessaire.

![Capture d'écran des paramètres de l'IA](https://github.com/user-attachments/assets/ad3c3467-debd-4632-aada-68e1291ab8f8)

Si vous souhaitez en savoir plus sur les paramètres **« temperature »** et **« top_p »**, [consultez cet article](https://medium.com/@1511425435311/understanding-openais-temperature-and-top-p-parameters-in-language-models-d2066504684f) (niveau avancé).

---

### 2. Sélectionner le modèle d'IA à utiliser

Dans QualCoder, vous pouvez choisir parmi différents modèles de langage de grande taille, et même [ajouter le vôtre](#utiliser-dautres-modèles-dia). Par défaut, les services suivants sont implémentés :

* **GPT-4 par OpenAI : meilleurs résultats, option recommandée, mais payante**
  * Pour utiliser GPT-4, vous aurez besoin d'une clé API d'OpenAI. Rendez-vous sur [https://platform.openai.com/](https://platform.openai.com/), créez un compte, accédez à votre tableau de bord personnel, cliquez sur **« API keys »** dans le menu de gauche, créez une clé, puis collez-la dans la boîte de dialogue des paramètres de QualCoder.
  * Bien que GPT-4 ne soit pas gratuit, il reste relativement peu coûteux. OpenAI facture un petit montant pour chaque requête, généralement quelques centimes. Vous devez acheter des **« crédits »** chez OpenAI avant de l'utiliser ; le montant minimum est de 5 $, ce qui permet une utilisation prolongée.
  * Notez qu'un abonnement ChatGPT Plus ne couvre pas l'utilisation dans QualCoder. Vous devez toujours acheter des crédits comme décrit ci-dessus.
  * QualCoder propose actuellement le choix entre **« GPT-4 turbo »** (toujours recommandé) et **« GPT-4o »**, un modèle plus récent, moins cher mais légèrement moins puissant. Les deux peuvent utiliser la même clé API.

* **Blablador : sans but lucratif, excellente confidentialité, gratuit, qualité moyenne**
  * Ce service est fourni par l'agence de recherche académique allemande [Helmholtz Society](https://www.helmholtz.de/en/). Il utilise des modèles ouverts de taille moyenne et est très respectueux de la vie privée, ne stockant aucune donnée. Pour plus d'informations, consultez cette [présentation d'Alexandre Strube](https://strube1.pages.jsc.fz-juelich.de/2024-02-talk-lips-blablador/).
  * Note sur la qualité : Comme Blablador utilise des modèles beaucoup plus petits que ceux d'OpenAI, les interprétations sont moins nuancées. Les grands modèles comme GPT-4 offrent une meilleure compréhension du contexte et peuvent même analyser des détails subtils et des significations implicites, ce qui est souvent crucial pour la recherche qualitative. De plus, l'utilisation de Blablador peut entraîner des bugs dans l'interface utilisateur, comme des réponses en anglais au lieu de la langue de l'utilisateur, ou des références sources mal formées. Pour l'instant, nous recommandons Blablador uniquement pour des questions simples, si vous souhaitez expérimenter avec des modèles ouverts, ou si vous avez absolument besoin d'une confidentialité renforcée (voir ci-dessous pour plus d'informations sur la [vie privée et la protection des données](#vie-privée-et-protection-des-données)).
  * Blablador est gratuit, mais nécessite toujours une clé API personnelle de la Helmholtz Society. Vous pouvez vous inscrire avec votre compte universitaire ou via GitHub, Google ou ORCID. Suivez les instructions ici : [https://sdlaml.pages.jsc.fz-juelich.de/ai/guides/blablador_api_access/](https://sdlaml.pages.jsc.fz-juelich.de/ai/guides/blablador_api_access/).

> [!IMPORTANT]
> À partir de novembre 2025, si vous utilisez QualCoder 3.7 ou une version antérieure, vous devez mettre à jour votre fichier **config.ini** pour retrouver l'accès à Blablador. En effet, le service a migré vers un nouveau serveur plus puissant. Suivez ces étapes pour pointer QualCoder vers le nouveau serveur :
> - Fermez QualCoder.
> - Ouvrez le fichier **config.ini** dans le dossier des paramètres de QualCoder.
>   - Sous Windows : `C:\Users\VOTRE_NOM_DUTILISATEUR\.qualcoder\config.ini`
>   - Sous Mac ou Linux : `~/.qualcoder/config.ini` (pour ouvrir ce dossier caché sur Mac, cliquez sur **Aller → Aller au dossier…** dans la barre de menu du Finder et entrez `~/.qualcoder`).
> - Dans le fichier, descendez jusqu'à la section `[ai_model_Blablador]`.
> - Modifiez la valeur de **api_base** de `https://helmholtz-blablador.fz-juelich.de:8000/v1` à la nouvelle adresse `https://api.helmholtz-blablador.fz-juelich.de/v1/`.
> - Enregistrez et fermez le fichier **config.ini**, puis redémarrez QualCoder.
>
> Nous corrigeons ce problème dans la prochaine version de QualCoder.

* **Pour d'autres options de fournisseurs d'IA, [voir ci-dessous](#utiliser-dautres-modeles-dia).**

---

### 3. Télécharger un modèle d'IA local supplémentaire

* En plus des grands modèles de langage décrits ci-dessus, QualCoder utilise un modèle d'IA local plus petit comme étape préliminaire de l'analyse pour limiter la quantité de données envoyées au cloud.
* Ce [modèle open-source](https://huggingface.co/intfloat/multilingual-e5-large), d'une taille d'environ 2,5 Go, sera automatiquement téléchargé et installé sur votre ordinateur la première fois que vous activerez l'IA.
* Sous Windows, le modèle est stocké ici : `C:\Users\VOTRE_NOM_DUTILISATEUR\.cache\torch\sentence_transformers`
* Sous Linux, le modèle est stocké ici : `/home/VOTRE_NOM_DUTILISATEUR/.cache/torch/sentence_transformers`
* Sous macOS, le modèle est stocké ici : `/users/VOTRE_NOM_DUTILISATEUR/.cache/torch/sentence_transformers`

---

### 4. Lire les documents empiriques dans la mémoire de l'IA

* Si vous avez un projet ouvert, le modèle d'IA local mentionné ci-dessus lira tous les documents texte du projet individuellement et les incorporera dans sa mémoire interne.
* Ce processus ne se produit qu'une seule fois. Lorsque vous rouvrirez le projet par la suite, cette **« mémoire IA »** se chargera rapidement depuis le disque. Seuls les nouveaux documents ajoutés au projet ou les documents modifiés seront relus par l'IA locale.
* L'ensemble du processus de lecture et de mémorisation de vos documents s'effectue en arrière-plan (la barre d'état affiche **« IA : lecture »**). Pendant ce temps, les fonctions liées à l'IA seront indisponibles, mais vous pourrez utiliser le reste de QualCoder normalement.

![Message « IA : lecture »](https://github.com/user-attachments/assets/04f268f9-d05e-431f-b4fd-d4eabf210a0b)

⚠ Utilisez ce temps pour **mettre à jour le mémo de votre projet** (Menu **« IA > Mémo du projet »**) avec une brève description des thèmes de recherche de votre projet, des questions, des objectifs et des données empiriques collectées. Ces informations sont très importantes, car elles accompagneront chaque requête envoyée à l'IA, ce qui permettra d'obtenir des résultats beaucoup plus ciblés.

Une fois toutes ces étapes terminées, la barre d'état de l'application affichera **« IA : prêt »**, et vous pourrez commencer à utiliser le [chat IA](https://qualcoder-org.github.io/doc/fr/5.1.-AI-chat-based-analysis) ou le [codage assisté par IA](https://qualcoder-org.github.io/doc/fr/4.2.-AI-Assisted-Coding) pour explorer vos données.

![Message « IA : prêt »](https://github.com/user-attachments/assets/31ae8bf0-fb14-492c-acb0-c0254cd38773)

Si vous souhaitez modifier ultérieurement le modèle d'IA et les paramètres, ou désactiver complètement les fonctionnalités d'IA, vous pouvez le faire en accédant à **« IA > Paramètres »**.

---

## Quelques remarques sur la vie privée et la protection des données

* L'utilisation de services basés sur le cloud, comme ceux d'OpenAI, soulève des questions concernant la confidentialité et la protection des données.
* Nous estimons cependant que l'utilisation de ces services dans QualCoder est conforme aux principes éthiques qui guident la recherche sociale qualitative. Sinon, nous n'offririons pas de telles fonctionnalités.
* La décision finale vous appartient, en fonction de votre projet spécifique et du type de données que vous manipulez. Nous nous efforçons de rendre le plus transparent possible ce qui arrive à vos données, afin que vous puissiez prendre une décision éclairée.

QualCoder suit une approche **« Privacy by Design »** :

* L'application **enverra le moins de données possible au cloud**, en utilisant un traitement local chaque fois que cela est possible. La plupart des fonctionnalités utilisent la mémoire IA locale décrite ci-dessus pour effectuer une présélection des données pertinentes. Ensuite, seuls un petit nombre de segments de texte sélectionnés (chacun d'environ 500 caractères) sont envoyés au cloud pour une analyse plus approfondie.
* Avec OpenAI, QualCoder utilise l **« accès API »** à GPT-4, qui est régi par les [règles de confidentialité Enterprise](https://openai.com/enterprise-privacy). Ces règles sont beaucoup plus strictes en matière de protection des données que ChatGPT. **OpenAI garantit que les données envoyées via cette interface [ne seront PAS utilisées pour entraîner des modèles d'IA](https://platform.openai.com/docs/models/how-we-use-your-data#how-we-use-your-data)**, mais seront conservées de manière confidentielle et supprimées dans les 30 jours.
* QualCoder n'est pas limité à l'utilisation d'OpenAI. Nous nous sommes associés à **« Blablador »**, un service exécutant des grands modèles de langage sur du matériel académique non commercial, offrant une excellente confidentialité (voir ci-dessus).
* Il est également possible [d'ajouter vos propres modèles à QualCoder](#utiliser-dautres-modeles-dia), y compris ceux fonctionnant entièrement en local sur votre machine sans aucun accès réseau. À ce jour, cependant, le compromis en termes de qualité de sortie est tel que les modèles d'IA locaux n'ont, selon nous, que des cas d'utilisation très limités. Espérons que cette situation changera à l'avenir.
* En général, nous vous recommandons d'**anonymiser soigneusement vos données** avant d'utiliser un service d'IA basé sur le cloud pour l'analyse.

---

## Utiliser d'autres modèles d'IA

Les modèles d'IA disponibles, sélectionnables dans la boîte de dialogue des paramètres, sont définis dans le fichier **« config.ini »**, situé dans le sous-dossier **« .qualcoder »** du répertoire personnel de l'utilisateur. Assurez-vous de ne modifier ce fichier que lorsque QualCoder est fermé, sinon vos modifications seront écrasées.

---

### Microsoft Azure

Si vous avez accès à GPT-4 (ou à d'autres modèles OpenAI) sur la plateforme cloud Microsoft Azure, vous pouvez également l'utiliser. Vous devez connaître :
- le nom de votre déploiement,
- l'URL de votre point de terminaison (doit être de la forme **« https://XXX.openai.azure.com/ »**),
- votre clé API.

Suivez ce tutoriel (notamment la section **« Retrieve key and endpoint »**) pour obtenir ces informations :
[Démarrage rapide : Commencer à utiliser GPT-35-Turbo et GPT-4 avec Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line%2Cjavascript-keyless%2Ctypescript-keyless%2Cpython-new&pivots=programming-language-python).
Ensuite, créez une nouvelle entrée dans le fichier **« config.ini »**, situé dans le sous-dossier **« .qualcoder »** du répertoire personnel de l'utilisateur, avec les informations suivantes :
```ini
[ai_model_GPT-4-Azure]
desc = GPT-4 déployé via Microsoft Azure
access_info_url =
large_model = NOM_DE_VOTRE_DÉPLOIEMENT_AZURE
large_model_context_window = 128000
fast_model = NOM_DE_VOTRE_DÉPLOIEMENT_AZURE
fast_model_context_window = 128000
api_base = URL_DE_VOTRE_POINT_DE_TERMINAISON_AZURE
api_key = VOTRE_CLÉ_API

```
### OpenRouter.ai

[OpenRouter](https://openrouter.ai/) est une interface unifiée permettant d'accéder à de nombreux grands modèles de langage sur différents serveurs. Certains de ces modèles sont gratuits, d'autres nécessitent un abonnement payant. La plupart des modèles gratuits ont des limites de quota strictes, ce qui peut entraîner des erreurs avec les modèles populaires.
Pour intégrer un modèle d'OpenRouter.ai dans QualCoder, créez une nouvelle entrée dans le fichier **« config.ini »** avec les informations suivantes :
```ini
[ai_model_OpenRouter_NOM_DU_MODÈLE]
desc = une description, peut être vide
access_info_url = https://openrouter.ai/
large_model = NOM_EXACT_DU_MODÈLE, par exemple : google/gemini-2.0-pro-exp-02-05\:free
large_model_context_window = 2000000
fast_model = nom d'un modèle plus petit et plus rapide, peut être identique à large_model
fast_model_context_window = 2000000
api_base = https://openrouter.ai/api/v1
api_key = VOTRE_CLÉ_API_OPENROUTER

```
### Google Gemini

Google offre différents modèles payants et gratuits sur ses serveurs. Exemple de configuration pour Gemini 1.5 flash:
```
[ai_model_Gemini_1.5_flash]
desc = Gemini 1.5 flash on Google Servers
access_info_url = https://ai.google.dev/gemini-api/docs/openai
large_model = gemini-1.5-flash
large_model_context_window = 2000000
fast_model = gemini-1.5-flash
fast_model_context_window = 2000000
api_base = https://generativelanguage.googleapis.com/v1beta/openai/
api_key = your google API key
```
### Local models using Ollama

[Ollama](https://ollama.com) est un choix populaire pour exécuter des modèles de langage localement sur votre propre machine. Il dispose également d'une [API compatible OpenAI](https://ollama.com/blog/openai-compatibility) qui peut être utilisée avec QualCoder.

Exemple de configuration pour QualCoder :
```
[ai_model_MODEL_NAME]
desc = A local model using Ollama 
access_info_url = https://ollama.com
large_model = <The exact name of the model in the API>
large_model_context_window = <The maximum number of tokens in a single request>
fast_model = <Can be identical to large_model or name a smaller model, used for simple tasks>
fast_model_context_window = <The maximum number of tokens for the small model>
api_base = http://localhost:11434/v1/
api_key = ollama
```
### Autres modèles

Si vous avez accès à d'autres modèles linguistiques volumineux (sur des plateformes cloud, des serveurs universitaires ou même votre propre machine), vous pouvez essayer de les intégrer à QualCoder.
* Le service que vous souhaitez utiliser doit fournir une interface compatible avec l'API OpenAI. C'est souvent le cas, car cette API est devenue une norme de facto au cours des derniers mois. 
* Les modèles disponibles dans QualCoder sont définis dans le fichier « config.ini », situé dans le sous-dossier « .qualcoder » du répertoire personnel de l'utilisateur. Les définitions de modèles ont le format suivant (omettez les balises « <> ») :


```
[ai_model_your_model_name]
desc = A description shown in the UI.
       Can have more than one line.
access_info_url = <URL pointing to a website with model info. Can be empty>
large_model = <The exact name of the model in the API>
large_model_context_window = <The maximum number of tokens in a single request>
fast_model = <Can be identical to large_model or name a smaller model, used for simple tasks>
fast_model_context_window = <The maximum number of tokens for the small model>
api_base = <The URL of the API base, e.g., http://localhost:11434/v1 for a local Ollama>
api_key = <The API-key if needed, or "None" instead. Do not leave this field empty.>
```
Notez que le nom de la section doit toujours commencer par le préfixe « ai_model_ ».

Si vous utilisez d'autres services dans QualCoder (ou si vous avez essayé sans succès), nous aimerions connaître vos expériences.
