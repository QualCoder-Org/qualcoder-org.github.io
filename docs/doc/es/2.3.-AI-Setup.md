---
path: 2.3.-AI-Setup
---

# Configuración de la IA
Tras completar la instalación de _QualCoder_, las funciones potenciadas por IA requieren una configuración adicional. Al iniciar la aplicación por primera vez, aparecerá un asistente de configuración (Setup wizard) que le guiará a través del proceso (Véase Imagen 1 e Imagen 3). Si decide omitir este asistente, puede reiniciarlo más tarde seleccionando la opción del menú "IA > Asistente de configuración" (Véase Imagen 2).

Imagen 1:

<img width="1141" height="463" alt="image" src="https://github.com/user-attachments/assets/c7a67506-4f4a-4cd9-ad66-26a7df57a9bd" />


Imagen 2:

<img width="1434" height="486" alt="image" src="https://github.com/user-attachments/assets/2b7a6517-ff59-47c1-a495-12e0e07c5e6f" />



El proceso de configuración se ejecuta de manera mayoritariamente automática y desatendida. Es un procedimiento que se realiza una sola vez y puede demorar cierto tiempo, por lo que se solicita su paciencia. Los pasos principales son los siguientes: 

### 1. Habilitar la IA

* Tenga en cuenta que las funciones de IA son de "aceptación voluntaria" (opt-in) y se encuentran deshabilitadas de forma predeterminada.
* Todas las opciones relacionadas con la IA se encuentran en la parte inferior de la ventana de Configuraciones. Desplácese hacia abajo según sea necesario.

Imagen 3: 

<img width="1047" height="383" alt="image" src="https://github.com/user-attachments/assets/ec6156e8-f390-43fd-9258-c08bfd48c812" />

**Modelos de IA**

Tabla 1:
| Modelo de IA  | Tipo de acceso | Enlace Clave API     |
| ------------- |:-------------:|:-------------:|
| OpenAI GPT5.2 reasoning      | Pago     | [OpenAI GPT5.2 reasoning](https://platform.openai.com/api-keys)     |
| OpenAI GPT5.2 no reasoning      | Pago     | [OpenAI GPT5.2 no reasoning](https://platform.openai.com/api-keys)     |
| Blablador      | Gratuita     | [Blablador](https://sdlaml.pages.jsc.fz-juelich.de/ai/guides/blablador_api_access/)     |
| Blablador Huge      | Gratuita     | [Blablador Huge](https://sdlaml.pages.jsc.fz-juelich.de/ai/guides/blablador_api_access/)     |
| Anthropic Claude Sonnet 4.5      | Pago     | [Anthropic Claude Sonnet 4.5](https://platform.claude.com/settings/keys)     |
| Google Gemini      | Gratuita y de Pago     | [Google Gemini](https://ai.google.dev/gemini-api/docs)     |
| Deepseek Chat V3      | Pago     | [Deepseek Chat V3](https://platform.deepseek.com/api_keys)     |
| Mistral      | Gratuita y de Pago     | [Mistral](https://mistral.ai/)     |
| OpenRouter      | Gratuita y de Pago     | [OpenRouter](https://openrouter.ai/)     |
| Ollama local AI      | Gratuita     | [Ollama local AI](https://ollama.com/) (Instalación local, NO requiere clave API)    |

Si desea obtener más información sobre los parámetros de "temperatura" (temperature) y "top_p" [consulte este artículo](https://medium.com/@1511425435311/understanding-openais-temperature-and-top-p-parameters-in-language-models-d2066504684f) (Avanzado).

### 2. Seleccione el modelo de IA que desee utilizar.

En _QualCoder_, puede elegir entre diferentes Modelos de Lenguaje de Gran Tamaño (LLM, por sus siglas en inglés) e incluso [añadir el suyo propio](#uso-de-otros-modelos-de-ia). Por defecto, se encuentran implementados los siguientes servicios:

* __GPT-4 de OpenAI: los mejores resultados, opción recomendada, aunque no es gratuita__ 
    * Para utilizar GPT-4, necesitará una clave API de OpenAI. Visite https://platform.openai.com/, cree una cuenta, navegue hasta su panel de control personal, haga clic en "API keys" en el menú de la izquierda, cree una clave y péguela en el cuadro de ingresar texto de la ventana de Configuraciones en _QualCoder_. 
    * Aunque GPT-4 no es gratuito, resulta relativamente económico. OpenAI cobra una pequeña tarifa por cada solicitud, que suele ser de apenas unos pocos céntimos. Es necesario adquirir "créditos" de OpenAI antes de utilizar el servicio; el importe mínimo es de $5 dólares, una cantidad que rinde considerablemente.
    * Tenga en cuenta que la suscripción a ChatGPT Plus no cubre el uso de la IA en _QualCoder_. Aun así, deberá adquirir créditos tal como se describió anteriormente.
    * _QualCoder_ ofrece actualmente la posibilidad de elegir entre "GPT-4 turbo" -que sigue siendo nuestra recomendación- y "GPT-4o", un modelo más reciente y económico, aunque ligeramente menos potente. Ambos pueden utilizar la misma clave API.

* __Blablador: sin fines de lucro, excelente privacidad, de uso gratuito, calidad media__
    * Este servicio es proporcionado por la agencia de investigación académica alemana [Helmholtz Society](https://www.helmholtz.de/en/). Utiliza modelos abiertos de tamaño medio y es sumamente respetuoso con la privacidad, ya que no almacena ningún tipo de dato. Para obtener más información, consulte esta [presentación de Alexandre Strube](https://strube1.pages.jsc.fz-juelich.de/2024-02-talk-lips-blablador/).
    * Una nota sobre la calidad: dado que Blablador utiliza modelos mucho más pequeños que OpenAI, sus interpretaciones presentan menos matices. Los modelos de mayor tamaño, como GPT-4, ofrecen una mejor comprensión del contexto e incluso pueden analizar, hasta cierto punto, detalles sutiles y significados implícitos, lo cual suele ser crucial para la investigación cualitativa. Además, el uso de Blablador puede ocasionar algunos fallos en la interfaz de usuario, tales como respuestas en inglés en lugar del idioma del usuario o referencias de origen mal estructuradas. Por el momento, recomendamos Blablador únicamente para preguntas sencillas, si desea experimentar con modelos abiertos o si requiere estrictamente un nivel adicional de privacidad (consulte más adelante la sección sobre [privacidad y protección de datos](#algunas-notas-sobre-la-privacidad-y-la-protecci%C3%B3n-de-datos)). 
    * Blablador es de uso gratuito, pero requiere una clave API personal de la Helmholtz Society. Puede registrarse utilizando su cuenta universitaria o a través de GitHub, Google o ORCID. Siga las instrucciones disponibles en este enlace: [https://sdlaml.pages.jsc.fz-juelich.de/ai/guides/blablador_api_access/](https://sdlaml.pages.jsc.fz-juelich.de/ai/guides/blablador_api_access/).

> [!IMPORTANTE]
> A partir de noviembre de 2025, si está utilizando _QualCoder 3.7_ o inferior, debe actualizar su archivo config.ini para recuperar el acceso a Blablador. Esto se debe a que el servicio ha migrado a un nuevo servidor más potente. Siga estos pasos para redirigir _QualCoder_ al nuevo servidor:
> - Cierre _QualCoder_
> - Abra el archivo config.ini ubicado en la carpeta de ajustes de _QualCoder_. 
>   - En Windows: `C:\Users\TU USUARIO\.qualcoder\config.ini` 
>   - En macOS o Linux: `~/.qualcoder/config.ini` (Para abrir esta carpeta oculta en un Mac, haga clic en Ir → Ir a la carpeta… en la barra de menús del Finder e introduzca: `~/.qualcoder`).
> - En el archivo, desplácese hacia abajo hasta la sección: `[ai_model_Blablador]` .
> - Cambie el valor de `api_base` de `https://helmholtz-blablador.fz-juelich.de:8000/v1` a la nueva dirección `https://api.helmholtz-blablador.fz-juelich.de/v1/`.
> - Guarde y cierre el archivo config.ini y, a continuación, reinicie _QualCoder_. 
>
> Corregiremos este problema en la próxima actualización de _QualCoder_.

* __Para otras opciones de proveedores de IA [véase más abajo](https://qualcoder-org.github.io/doc/es/2.3.-AI-Setup/_edit#uso-de-otros-modelos-de-ia).__


### 3. Descarga de un modelo de IA local adicional

* Además de los Modelos de Lenguaje de Gran Tamaño descritos anteriormente, _QualCoder_ también utiliza un modelo de IA local de menor tamaño como paso preliminar en el análisis, con el fin de limitar la cantidad de datos que deben enviarse a la nube. 
* Este [modelo de código abierto](https://huggingface.co/intfloat/multilingual-e5-large), con un tamaño aproximado de 2.5 GB, se descargará e instalará automáticamente en su ordenador la primera vez que habilite la IA.
* En Windows, el modelo se almacena aquí: C:\\Users\\YOUR_USERNAME\\.cache\\torch\\sentence_transformers
* En Linux, el modelo se almacena aquí: /home/YOUR_USERNAME/.cache/torch/sentence_transformers
* En macOS, el modelo se almacena aquí: /users/YOUR_USERNAME/.cache/torch/sentence_transformers

### 4. Lectura de los documentos empíricos en la memoria de la IA

* Si tiene un proyecto abierto, el modelo de IA local mencionado anteriormente leerá ahora todos los documentos de texto del proyecto de forma individual y los incorporará a su memoria interna. 
* Este proceso ocurre una sola vez. Cuando vuelva a abrir el proyecto posteriormente, esta "memoria de la IA" se cargará rápidamente desde el disco. Solo si añade nuevos documentos a su proyecto o edita los existentes, la IA local volverá a leer dichos documentos.
* Todo el proceso de lectura y memorización de sus documentos ocurre en segundo plano (la barra de estado mostrará "IA: Leyendo datos") (Véase Imagen 4). Mientras esto sucede, las funciones relacionadas con la IA no estarán disponibles, pero puede utilizar el resto de _QualCoder_ con normalidad.

Imagen 4: 

<img width="346" height="159" alt="image" src="https://github.com/user-attachments/assets/03eb3b12-8289-466e-9949-7ec6aebcac5a" />

⚠ Aproveche este tiempo para actualizar el [**memo de su proyecto**](#memo-del-proyecto) con una breve descripción de los temas de investigación, preguntas, objetivos y los datos empíricos recopilados. Esta información es sumamente importante, ya que acompañará a cada instrucción (prompt) enviada a la IA, lo que permitirá obtener resultados mucho más precisos y específicos.

>> ### Memo del proyecto
>>El memo del proyecto en QualCoder es una pieza metodológica central que funciona como un diario analítico para garantizar la transparencia del proceso y consolidar la información relevante de su investigación, la cual resulta sumamente importante para contextualizar cada instrucción enviada a la IA.
>>
>> #### Puede acceder al Memo del proyecto de tres maneras distintas:
>> 
>> **1. Atajo de teclado**: Presionando simultáneamente las teclas **CTRL+M**
>> 
>> **2. Menú _Proyecto_**: Seleccionando la opción "Memo de proyecto" dentro de la pestaña desplegable (Véase Imagen 5).
>>    
>>Imagen 5:
>>
>> <img width="771" height="405" alt="image" src="https://github.com/user-attachments/assets/f2e23b59-9f29-4776-93d3-1cfab7e7bc04" />
>>
>> 
>> **3. Menú _IA_**: Seleccionando la opción "Memo de proyecto" dentro de la pestaña desplegable (Véase Imagen 6).
>>    
>>Imagen 6: 
>>
>> <img width="775" height="334" alt="image" src="https://github.com/user-attachments/assets/8c145ab1-f0f8-4f85-adda-d9f96e3276a9" />
>>
>> 
>> **Elementos principales del Memo de proyecto (Véase Imagen 7)**:
>>
>>Imagen 7:
>>
>> <img width="651" height="513" alt="image" src="https://github.com/user-attachments/assets/a5684ef6-3cd0-46b4-a9dd-a4999c2d45f6" />
>>
>> 




Una vez completado el proceso de lectura, la barra de estado de la aplicación mostrará el mensaje "IA: ready" (Véase Imagen 8) y podrá comenzar a utilizar el [chat de IA](https://qualcoder-org.github.io/doc/es/5.1.-AI-chat-based-analysis) o la [codificación asistida por IA](https://qualcoder-org.github.io/doc/es/4.2.-AI-Assisted-Coding) para explorar sus datos.

Imagen 8:

<img width="343" height="159" alt="image" src="https://github.com/user-attachments/assets/8babdcbf-ebdb-4860-b4dc-d605a754517c" />

Si desea cambiar posteriormente el modelo de IA y su configuración, o desactivar las funciones de IA por completo, puede hacerlo dirigiéndose al menú IA > Configuraciones.

## Algunas notas sobre la privacidad y la protección de datos

* El uso de servicios basados en la nube, como los de OpenAI, plantea interrogantes en relación con la privacidad y la protección de los datos. 

* No obstante, consideramos que el uso que _QualCoder_ hace de estos servicios está en consonancia con los principios éticos que rigen la investigación social cualitativa. De lo contrario, no ofreceríamos dichas funciones. 

* La decisión final recae en usted y dependerá, en gran medida, de su proyecto específico y del tipo de datos que maneje. Nuestro objetivo es que el tratamiento de sus datos sea lo más transparente posible, permitiéndole así tomar una decisión informada.  

_QualCoder_ sigue un enfoque de **privacidad desde el diseño**:  

* La aplicación **enviará la menor cantidad posible de datos a la nube**, utilizando el procesamiento local siempre que sea factible. La mayoría de las funciones emplean la memoria de IA local descrita anteriormente para realizar una preselección de los datos relevantes. Posteriormente, solo se envía a la nube un número reducido de fragmentos de texto seleccionados (cada uno de aproximadamente 500 caracteres) para un análisis más profundo.  

* Con OpenAI, _QualCoder_ utiliza el "acceso vía API" a GPT-4, el cual se rige por las [regulaciones de privacidad para empresas](https://openai.com/enterprise-privacy). Estas regulaciones son mucho más estrictas en términos de protección de datos en comparación con ChatGPT. **OpenAI garantiza que los datos enviados a través de esta interfaz [NO se utilizarán para entrenar modelos de IA](https://platform.openai.com/docs/models/how-we-use-your-data#how-we-use-your-data)** sino que se mantendrán de forma confidencial y se eliminarán en un plazo de 30 días.

* _QualCoder_ no se limita al uso de OpenAI. Nos hemos asociado con **Blablador**, un servicio que ejecuta modelos de lenguaje de gran tamaño (LLM) en hardware académico no comercial, el cual ofrece niveles de privacidad excelentes (véase más arriba). 

* Adicionalmente, es posible [añadir sus propios modelos a _QualCoder_](#uso-de-otros-modelos-de-ia), incluidos aquellos que se ejecutan de forma totalmente local en su computadora sin ningún tipo de acceso a la red. Sin embargo, a día de hoy, el compromiso en la calidad de los resultados es tan significativo que, en nuestra opinión, los modelos de IA locales tienen casos de uso muy limitados. Esperamos que esta situación cambie en el futuro.  

* En general, recomendamos que **anonimice sus datos** de manera muy cuidadosa antes de utilizar cualquier servicio de IA basado en la nube para el análisis.  

## Uso de otros modelos de IA

Los modelos de IA disponibles, seleccionables en el cuadro de diálogo de configuración, se definen en el archivo "config.ini", ubicado en la subcarpeta ".qualcoder" dentro del directorio de inicio del usuario. Asegúrese de editar este archivo únicamente si _QualCoder_ está cerrado; de lo contrario, sus cambios se sobrescribirán. 

### Microsoft Azure

Si tiene acceso a GPT-4 (u otros modelos de OpenAI) en la plataforma en la nube Azure de Microsoft, también puede utilizarlos. Para ello, necesitará conocer su: 
-	deployment name,
-	URL del punto de conexión (debe ser algo parecido a "https://XXX.openai.azure.com/"), y
-	API-key.

Siga este tutorial (especialmente la sección "Retrieve key and endpoint") para obtener esta información: 
[Quickstart: Get started using GPT-35-Turbo and GPT-4 with Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line%2Cjavascript-keyless%2Ctypescript-keyless%2Cpython-new&pivots=programming-language-python).
A continuación, cree una nueva entrada en el archivo "config.ini", ubicado en la subcarpeta ".qualcoder" dentro del directorio de inicio del usuario, con la siguiente información:
```
[ai_model_GPT-4-Azure]
desc = GPT-4 deployed via Microsoft Azure
access_info_url =
large_model = YOUR AZURE DEPLOYMENT NAME
large_model_context_window = 128000
fast_model = YOUR AZURE DEPLOYMENT NAME
fast_model_context_window = 128000
api_base = YOUR AZURE ENDPOINT URL 
api_key = YOUR API KEY
```
### OpenRouter.ai

[OpenRouter](https://openrouter.ai/) es una interfaz unificada a través de la cual se puede acceder a múltiples modelos de lenguaje de gran tamaño (LLM) en diferentes servidores. Algunos de estos modelos son gratuitos, mientras que otros requieren una suscripción de pago. La mayoría de los modelos gratuitos tienen límites de cuota de uso estrictos, lo que puede provocar errores al utilizar modelos populares. 
Para integrar un modelo de OpenRouter.ai en _QualCoder_, cree una nueva entrada en el archivo «config.ini» con la siguiente información:
```
[ai_model_OpenRouter_MODELNAME]
desc = some description, can be empty
access_info_url = https://openrouter.ai/
large_model = THE EXACT MODEL NAME, e.g.: google/gemini-2.0-pro-exp-02-05:free
large_model_context_window = 2000000
fast_model = the name of a smaller, faster model, can be the same as large_model
fast_model_context_window = 2000000
api_base = https://openrouter.ai/api/v1
api_key = YOU OPENROUTER API KEY
```
### Google Gemini

Google ofrece diversos modelos gratuitos y de pago en sus servidores. A continuación, se presenta un ejemplo de configuración para Gemini 1.5 Flash:
```
[ai_model_Gemini_1.5_flash]
desc = Gemini 1.5 flash on Google Servers
access_info_url = https://ai.google.dev/gemini-api/docs/openai
large_model = gemini-1.5-flash
large_model_context_window = 2000000
fast_model = gemini-1.5-flash
fast_model_context_window = 2000000
api_base = https://generativelanguage.googleapis.com/v1beta/openai/
api_key = your google API key
```
### Local models using Ollama

(Ollama)[https://ollama.com] es una opción popular para ejecutar modelos de lenguaje de forma local en su propia computadora. También incluye una (API compatible con OpenAI)[https://ollama.com/blog/openai-compatibility] que puede utilizarse con _QualCoder_.
ejemplo de configuración para _QualCoder_:
```
[ai_model_MODEL_NAME]
desc = A local model using Ollama 
access_info_url = https://ollama.com
large_model = <The exact name of the model in the API>
large_model_context_window = <The maximum number of tokens in a single request>
fast_model = <Can be identical to large_model or name a smaller model, used for simple tasks>
fast_model_context_window = <The maximum number of tokens for the small model>
api_base = http://localhost:11434/v1/
api_key = ollama
```
### Otros modelos

Si tiene acceso a otros modelos de lenguaje de gran tamaño (LLM) -ya sea en plataformas en la nube, servidores universitarios o incluso en su propia computadora- puede intentar integrarlos en _QualCoder_. 
* El servicio que desee utilizar debe proporcionar una interfaz compatible con la API de OpenAI. Este es el caso con frecuencia, dado que dicha API se ha convertido en un estándar de facto durante los últimos meses. 
* Los modelos disponibles en _QualCoder_ se definen en el archivo "config.ini," ubicado en la subcarpeta ".qualcoder" dentro del directorio de inicio del usuario. Las definiciones de los modelos tienen el siguiente formato (omita los signos "<>" ):
```
[ai_model_your_model_name]
desc = A description shown in the UI.
       Can have more than one line.
access_info_url = <URL pointing to a website with model info. Can be empty>
large_model = <The exact name of the model in the API>
large_model_context_window = <The maximum number of tokens in a single request>
fast_model = <Can be identical to large_model or name a smaller model, used for simple tasks>
fast_model_context_window = <The maximum number of tokens for the small model>
api_base = <The URL of the API base, e.g., http://localhost:11434/v1 for a local Ollama>
api_key = <The API-key if needed, or "None" instead. Do not leave this field empty.>
```
Tenga en cuenta que el nombre de la sección debe comenzar siempre con el prefijo "**ai_model_**".

Si logra ejecutar otros servicios en _QualCoder_ (o si lo intentó pero no tuvo éxito), nos gustaría conocer sus experiencias y esfuerzos al respecto.
