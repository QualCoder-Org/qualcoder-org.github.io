---
path: 2.3.-AI-Setup
---

# AI Setup
After the installation of QualCoder is completed, the AI-enhanced features require some additional setup. When you start the app for the first time, a **wizard** will appear and guide you through the process. If you choose to skip this wizard, you can restart it later by selecting the menu option "AI > Setup Wizard."

The entire setup process runs mostly automatically and unsupervised. It is a once-off process that may take some time, so please be patient. The main steps are: 

### 1. Enable the AI

* Note that the AI features are "opt-in" and disabled by default. 
* All AI-related options are located at the bottom of the settings dialog. Scroll down as needed.

![Screenshot AI Settings](https://github.com/user-attachments/assets/ad3c3467-debd-4632-aada-68e1291ab8f8)

If you want to learn more about the 'temperature' and 'top_p' settings, [check out this article](https://medium.com/@1511425435311/understanding-openais-temperature-and-top-p-parameters-in-language-models-d2066504684f) (advanced).

### 2. Select the AI model you want to use

In QualCoder, you can choose between different AI Large Language Models and even [add your own](#using-other-ai-models). By default, the following services are implemented:

* __GPT-4 by OpenAI: best results, recommended option, albeit not free__ 
    * To use GPT-4, you will need an API key from OpenAI. Visit https://platform.openai.com/, create an account, navigate to your personal dashboard, click on 'API keys' in the left menu, create a key, and paste it into the QualCoder settings dialog. 
    * Although GPT-4 is not free, it is relatively inexpensive. OpenAI charges a small fee for each request, typically just a few cents. You must purchase "credits" from OpenAI before using it; $5 is the minimum amount, which will go a long way.
    * Note that a ChatGPT Plus subscription does not cover usage in QualCoder. You must still purchase credits as described above.
    * QualCoder currently offers the choice between "GPT-4 turbo"—which we still recommend—and "GPT-4o", a newer, cheaper, but slightly less powerful model. Both can use the same API key.

* __Blablador: non-profit, excellent privacy, free to use, medium quality__
    * This service is provided by the German academic research agency [Helmholtz Society](https://www.helmholtz.de/en/). It runs medium sized open models, and it is very privacy-friendly, storing no data at all. For more information, see this [presentation from Alexandre Strube](https://strube1.pages.jsc.fz-juelich.de/2024-02-talk-lips-blablador/).
    * A note on quality: Since Blablador utilizes much smaller models than OpenAI, the interpretations are less nuanced. Larger models like GPT-4 provide better context awareness and can even analyze subtle details and implicit meaning to some extent, which is often crucial for qualitative research. Additionally, using Blablador may lead to some glitches in the user interface, such as responses in English instead of the user's language, or malformed source references. For now, we recommend Blablador only for simpler questions, if you wish to experiment with open models, or if you absolutely need the extra privacy (see below for more information on [privacy and data protection](#privacy-and-data-protection)). 
    * Blablador is free to use, but still requires a personal API key from the Helmholtz Society. You can sign up with your university account or with GitHub, Google, or ORCID. Follow the instructions here: [https://sdlaml.pages.jsc.fz-juelich.de/ai/guides/blablador_api_access/](https://sdlaml.pages.jsc.fz-juelich.de/ai/guides/blablador_api_access/).

> [!IMPORTANT]
> As of November 2025, if you are using QualCoder 3.7 or lower, you must update your config.ini file in order to regain access to Blablador. This is because the service has switched to a new, more powerful server. Follow these steps to point QualCoder to the new server
> - Close QualCoder
> - Open the file `config.ini` in the QualCoder settings folder. 
>   - On Windows: `C:\Users\YOUR USERNAME\.qualcoder\config.ini` 
>   - On Mac or Linux: `~/.qualcoder/config.ini` (to open this hidden folder on a Mac, click Go → Go to Folder… in the Finder menu bar and enter `~/.qualcoder`)
> - In the file, scroll down to the `[ai_model_Blablador]` section.
> - Change the value for `api_base` from `https://helmholtz-blablador.fz-juelich.de:8000/v1` to the new address `https://api.helmholtz-blablador.fz-juelich.de/v1/`.
> - Save and close the file config.ini, then restart QualCoder. 
>
> We will fix this issue in the next QualCoder release.

* __For other AI provider options [see below](https://qualcoder-org.github.io/doc/en/2.3.-AI-Setup/_edit#using-other-ai-models).__


### 3. Downloading an additional local AI model

* Beside the Large Language Models described above, QualCoder also uses a smaller, local AI model as a preliminary step in the analysis to limit the amount of data that must be sent to the cloud. 
* This [open-source model](https://huggingface.co/intfloat/multilingual-e5-large), approximately 2.5 GB in size, will be automatically downloaded and installed on your computer the first time you activate the AI.
* On Windows the model is stored here: C:\\Users\\YOUR_USERNAME\\.cache\\torch\\sentence_transformers
* On Linux the model is stored here: /home/YOUR_USERNAME/.cache/torch/sentence_transformers
* On macOS the model is stored here: /users/YOUR_USERNAME/.cache/torch/sentence_transformers

### 4. Reading the empirical documents into the AI memory

* If you have a project open, the aforementioned local AI model will now read through all text documents in the project individually and incorporate them into its internal memory. 
* This process occurs only once. When you reopen the project subsequently, this 'AI memory' will load from the disk quickly. Only if you add new documents to your project or edit existing ones will these documents be read by the local AI (again).
* The entire process of reading and memorizing your documents occurs in the background (status bar showing "AI: reading"). While this is happening, AI-related functions will be unavailable, but you can use the rest of QualCoder as usual.

![AI: reading message](https://github.com/user-attachments/assets/04f268f9-d05e-431f-b4fd-d4eabf210a0b)

⚠ Use the time to **update your project memo** (Menu "AI > Project Memo") with a short description of your project's research topics, questions, objectives, and the empirical data collected. This information is very important, as it will accompany every prompt sent to the AI, resulting in much more targeted results.

Once all these steps are completed, the status bar of the app will display "AI: ready," and you can begin using the [AI chat](https://qualcoder-org.github.io/doc/en/5.1.-AI-chat-based-analysis) or [AI assisted coding](https://qualcoder-org.github.io/doc/en/4.2.-AI-Assisted-Coding) to explore your data.

![AI: ready message](https://github.com/user-attachments/assets/31ae8bf0-fb14-492c-acb0-c0254cd38773)

If you wish to later change the AI model and settings or disable the AI features altogether, you can do so by navigating to "AI > Settings."

## Some Notes on Privacy and Data Protection

* Using cloud-based services like those from OpenAI raises questions regarding data privacy and protection. 

* However, we believe that QualCoder's use of these services is in alignment with the ethical principles that guide qualitative social research. Otherwise, we would not offer such features. 

* The final decision rests with you, depending heavily on your specific project and the type of data you handle. We aim at making it as transparent as possible what happens to your data, enabling you to make an informed decision.  

QualCoder follows a **"Privacy by Design"** approach:  

* The app will **send as little data as possible to the cloud,** utilizing local processing whenever feasible. Most features use the local AI memory described above to make a preselection of relevant data. Subsequently, only a small number of selected text chunks (each about 500 characters long) are sent to the cloud for deeper analysis.  

* With OpenAI, QualCoder utilizes the "API access" to GPT-4, which is governed by the [Enterprise privacy regulations](https://openai.com/enterprise-privacy). These regulations are much stricter in terms of data protection compared to ChatGPT. **OpenAI guarantees that the data sent via this interface [will NOT be used to train AI models](https://platform.openai.com/docs/models/how-we-use-your-data#how-we-use-your-data)** but will be kept confidential and deleted within 30 days.

* QualCoder is not limited to using OpenAI. We have partnered with **"Blablador,"** a service running Large Language Models on non-commercial, academic hardware which offers excellent privacy (see above). 

* Additionally, it is possible to [add your own models to QualCoder](#using-other-ai-models), including those running entirely locally on your machine without any network access. As of today, however, the trade-off in output quality is so significant that local AI models have only very limited use cases, in our opinion. This situation will hopefully change in the future.  

* In general, we recommend that you **anonymize your data** very carefully before using any cloud-based AI service for analysis.  

## Using Other AI Models

The available AI models, selectable in the settings dialog, are defined "config.ini" file, located in the ".qualcoder" subfolder in the user's home directory. Make sure to only edit this file if QualCoder is closed, or your changes will be overwritten. 

### Microsoft Azure

If you have access to GPT-4 (or other OpenAI models) on Microsoft’s cloud platform Azure, you can use this too. You need to know your 
-	deployment name,
-	endpoint URL (must be something like "https://XXX.openai.azure.com/"), and
-	API-key.

Follow this tutorial (especially the section "Retrieve key and	endpoint") to get this information: 
[Quickstart: Get started using GPT-35-Turbo and GPT-4 with Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line%2Cjavascript-keyless%2Ctypescript-keyless%2Cpython-new&pivots=programming-language-python).
Then create a new entry in the "config.ini" file, located in the ".qualcoder" subfolder in the user's home directory, with the following information:
```
[ai_model_GPT-4-Azure]
desc = GPT-4 deployed via Microsoft Azure
access_info_url =
large_model = YOUR AZURE DEPLOYMENT NAME
large_model_context_window = 128000
fast_model = YOUR AZURE DEPLOYMENT NAME
fast_model_context_window = 128000
api_base = YOUR AZURE ENDPOINT URL 
api_key = YOUR API KEY
```
### OpenRouter.ai

[OpenRouter](https://openrouter.ai/) is a unified interface through which you can access many Large Language Models on different servers. Some of these models are free, others require a paid subscription. Most of the free models have strict quota limits for usage, which can lead to errors with popular models. 
To integrate a model on OpenRouter.ai into QualCoder, create a new entry in the "config.ini" file with the following information:
```
[ai_model_OpenRouter_MODELNAME]
desc = some description, can be empty
access_info_url = https://openrouter.ai/
large_model = THE EXACT MODEL NAME, e.g.: google/gemini-2.0-pro-exp-02-05:free
large_model_context_window = 2000000
fast_model = the name of a smaller, faster model, can be the same as large_model
fast_model_context_window = 2000000
api_base = https://openrouter.ai/api/v1
api_key = YOU OPENROUTER API KEY
```
### Google Gemini

Google offers several free and paid models on their servers. Example config for Gemini 1.5 flash:
```
[ai_model_Gemini_1.5_flash]
desc = Gemini 1.5 flash on Google Servers
access_info_url = https://ai.google.dev/gemini-api/docs/openai
large_model = gemini-1.5-flash
large_model_context_window = 2000000
fast_model = gemini-1.5-flash
fast_model_context_window = 2000000
api_base = https://generativelanguage.googleapis.com/v1beta/openai/
api_key = your google API key
```
### Local models using Ollama

(Ollama)[https://ollama.com] is a popular choice for running Language Models locally on your own machine. It also comes with an (OpenAI compatible API)[https://ollama.com/blog/openai-compatibility] which can be used with Qualcoder.
Example config for QualCoder:
```
[ai_model_MODEL_NAME]
desc = A local model using Ollama 
access_info_url = https://ollama.com
large_model = <The exact name of the model in the API>
large_model_context_window = <The maximum number of tokens in a single request>
fast_model = <Can be identical to large_model or name a smaller model, used for simple tasks>
fast_model_context_window = <The maximum number of tokens for the small model>
api_base = http://localhost:11434/v1/
api_key = ollama
```
### Other models

If you have access to other Large Language Models—on cloud platforms, university servers, or even your own machine—you can try integrating them into QualCoder. 
* The service you wish to use must provide an interface compatible with the OpenAI API. This is often the case, since this API has become a de facto standard over the past months. 
* The available models in QualCoder are defined in the file "config.ini," located in the ".qualcoder" subfolder in the user's home directory. Model definitions have the following format (omit the "<>" marks):
```
[ai_model_your_model_name]
desc = A description shown in the UI.
       Can have more than one line.
access_info_url = <URL pointing to a website with model info. Can be empty>
large_model = <The exact name of the model in the API>
large_model_context_window = <The maximum number of tokens in a single request>
fast_model = <Can be identical to large_model or name a smaller model, used for simple tasks>
fast_model_context_window = <The maximum number of tokens for the small model>
api_base = <The URL of the API base, e.g., http://localhost:11434/v1 for a local Ollama>
api_key = <The API-key if needed, or "None" instead. Do not leave this field empty.>
```
Note that the section name must always start with the prefix 'ai_model_'.

If you get other services running in QualCoder (or if you tried but failed), we would like to hear about your endeavors.
